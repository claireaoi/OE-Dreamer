
<!DOCTYPE html>
<html lang="en">


<head>
  <meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <link rel="stylesheet" type="text/css" href="css/style.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.0.17/webcomponents-loader.js"></script>
  <script src="twgl.min.js"></script>
  <link rel="stylesheet" href="https://distill.pub/third-party/katex/katex.min.css" crossorigin="anonymous">
      
  <script src="https://distill.pub/template.v2.js"></script>
  <script src="https://distill.pub/third-party/highlight/highlight.pack.js"></script>

  <title>Open Ended Dreamer</title>
    
    <link rel="canonical" href="https://claireaoi.github.io/OE-dreams/">
    
    <!--  https://schema.org/Article -->

    <!--  https://scholar.google.com/intl/en/scholar/inclusion.html#indexing -->


</head>

<body distill-prerendered>


  <d-title>
    <h1 style="text-align: center;">Open Ended Dreamer</h1>
    <p>Open-Ended Library Learning in Unsupervised Program Synthesis</p>

    <p>
    </p><figure style="
        margin-left: auto;
        margin-right: auto;
        grid-column: page;
        width: 100%;
        max-width: 2000px;
      ">
      <img src="figures/Elites_nov.jpeg" style="width: 600px">
      <img src="figures/Elites_innate.jpeg" style="width: 600px">
      <figcaption> Extract of Elites Structures for a NOVEL resp. INNATE run.</figcaption>
    </figure>
  <p></p>

  </d-title>

  <d-abstract>
    <p>
      Pairing a neuro-symbolic model with library learning to facilitate <i>program induction</i> seems a 
      promising way of fostering open-ended innovation, by leveraging the robustness, expressivity, 
      and extrapolative capabilities of programs. This paper investigates how Open-Ended Dreamer (OED), 
      an unsupervised diversity-oriented neuro-symbolic learner built upon DreamCoder <d-cite key="ellis2021dreamcoder"></d-cite>, 
      may support open-ended program discovery. By alternating between phases of generation, selection, and abstraction, 
      OED aims to expand a hierarchical library of diversity-enabling building blocks (in the form of programs), 
      which are subsequently reused and composed in later iterations. As a first test-bed, we apply OED to a tower building
       domain and investigate the impact of library learning, neural guidance, innate priors, and language 
       or environmental pressures on the formation of symbolic knowledge. Our experiments suggest that promoting 
       greater exploration and stochasticity is crucial to offset the bias introduced by the growing language, 
       and foster more creative divergence.
    </p>
  </d-abstract>

  <d-byline>
  <div class="byline grid">
    <div>
      <h3>Authors</h3>
      <br>
        <p>Claire Glanois</p>
        <br>
        <p></p> Shyam Sudhakaran</p>

        <br>
        <p>Elias Najarro</p>
        <br>
        <p>Sebastian Risi</p>
        

      
    </div>
    <div>
      <h3>Affiliation</h3>
    
        <p>IT University of Copenhagen</p>
    </div>

    <div>
      <h3>Published</h3>
      <p>at Alife 2023</p> 
    </div>
  </div>
</d-byline>

<d-article>
<d-contents>
  <nav class="l-text toc figcaption">
    <h3>Contents</h3>
    <div><a href="#model">Model</a></div>
    <div><a href="#experiment-0">Tower Domain Experiments</a></div>
    <ul>
      <li><a href="#experiment-1">Experiment 1: Bias Experiment</a></li>
      <li><a href="#experiment-2">Experiment 2: Novelty-Diversity Trade Off</a></li>
      <li><a href="#experiment-3">Experiment 3: Ablation Experiment </a></li>
    </ul>
    <div><a href="#discussion">Discussion</a></div>
  </nav>
</d-contents>



  <p>
   Open-ended refers to a process that produces increasingly
    diverse and complex artifacts or behaviors. An archetypal
    inspiration are evolutionary processes which, through iterations 
    blending variation and selection mechanisms, have
    brought a stunningly diverse array of lifeforms on Earth.
    Most of these lifeforms showcase a complex nested and
    modular structure, whose primitive components can be
    found across numerous other species.
    Similarly, a significant part of human knowledge (factual
    or even procedural) exhibit some form of hierarchical and
    compositional structure, as we relentlessly invent more complex 
    artifacts and behaviors from previous stepping stones
    —explicit or implicit, collective or individual. This ability
    to re-use, combine, transfer or scale previous discoveries is
    crucial to both human learning and adaptation.
    Methods combining symbolic component with neural
    learning have shown great potential in grasping forms of
    compositional generalisation, while providing interpretable
    representations (Chen et al., 2020; De Raedt et al., 2019).
    For instance, a neuro-symbolic model like DreamCoder (Ellis et al., 2021) h
    as proven to be versatile in tackling multiple domains via program induction<d-footnote>
    Programs are an adaptive, robust and expressive way to encode
    both artifacts and behaviors, compelling for their logical, interpretative and extrapolative qualities.</d-footnote>
    ,from creative tasks as logo generation to more classic programming inductive 
    tasks like physical laws regression.
    This work extends DreamCoder towards open-endedness, with ways to encourage diversity 
    in both program generation and abstraction, but also making it less dependent on
    supervision. Our diversity-oriented neuro-symbolic model—Open-Ended Dreamer (OED)— 
    balances efficiency and novelty pressures in order to learn a compact and diverse
    set of reusable programs, organised in a hierarchical library.

    


  </p>

  <h2 id="model">Model</h2>
  <p>
    As DreamCoder, OED consists of both a neural and a symbolic component bootstrapping each other:
  L, hierarchical symbolic library of programs, capturing regularities in elites across niches
  Q, neural model guiding the search for efficient and novel programs.
  As schematized in Figure 1, OED learns
  through an iterative process alternating generation, selection, abstraction and training steps: at each iteration, elites
  programs are selected and organised by niches, before possibly being abstracted in the library-learning stage. The
  core differences with DreamCoder, detailed in the following paragraphs, are meant to encourage diversity both in
  the generation and abstraction phase, such as: (i) supervision relaxation via QD framework (ii) controllable
  exploration and stochasticity in the generation phase,
  (iii) novelty-selection phase (iv) stochastic pruning and
  novelty-filtering in the abstraction phase.

  </p>


  <p>
    </p><figure style="
        margin-left: auto;
        margin-right: auto;
        grid-column: page;
        width: 100%;
        max-width: 2000px;
      ">
      <img src="figures/OED.png" style="width: 600px">
      <figcaption> Open-Ended Dreamer follows an open-ended
        neuro-symbolic iterative process. (1) Neurally Guided Generation: by auto-regressively querying the neural model QL,
        programs are generated for each niche; (2) Novelty Selection: most novel programs are selected to form the elites (3) Abstraction across niches: refactored elites are
        abstracted to grow a library L of programmatic abstractions;
        (4) Neural Training: consolidate the neural guidance on both
        replays and sampled programs.</figcaption>
    </figure>
  <p></p>



  <p>
    <strong>Supervision Relaxation via Quality Diversity</strong> 
     By adopting a QD approach, OED relaxes DreamCoder’s supervision needs by eliminating the requirement for handpicking
    tasks. Following MAP-Elites (Mouret and Clune, 2015),
    OED keeps track of a multi-dimensional archive of phenotypic elites A, cut into niches along some predefined behavioral characteristics. In our experiments, we use 3D behavioral features (G, W, B) composed of gravity-center (height
    of) G, width W, and number of blocks B of the structure
    generated. Each dimension is split in 4, to create a total of
    4 ∗ 4 ∗ 4 = 64 niches. For instance, a program belongs
    to the niche [0, 2, 1] if its gravity center (y-coordinate) is
    ∈ [0, MG/4[, its width ∈ [MW /2, 3MW /4[ and its number of blocks ∈ [MB/4, MB/2[, where MG, MW , MB are
    upper boundaries set for each feature dimension (e.g. 120,
    240 and 400 in our experiments).
  </p>



  <p>
    <strong>Generation with Exploration</strong> 
  Both a top-down generation phase following the
generative model induced by the library L, and a bottomup generation phase 
following the conditional neural model
Q_L takes place at different stages in each iteration. Each
generation phase encompassing an enumeration procedure
and a sampling procedure. In the enumeration, similarly to
DreamCoder, programs ρ are enumerated in decreasing posterior probability order in each niche:
arg max ρ∈z P(ρ | z, L) ∝ P(z | ρ) | {z } if fits in the niche P(ρ | L) | {z }
how likely in library , (1) where z denotes a niche, ρ a program, L the library. The
likelihood P(z | ρ) evaluates if a program ρ fits in a given
niche z (i.e. 1 if it belongs to the niche, 0 else, as we opt
for a discrete niche criteria), and the prior P(ρ | L) reflects
how they fit in the library, i.e. how likely they are under the
generative model L. More details about this enumeration
procedure, which mixes best-first search with depth-first can
be found in DreamCoder (Ellis et al., 2021), Algorithm 3.
To encourage further exploration, OED incorporates a
sampling from the library L (for the top down phase) or from
neural model Q (for the bottom up phase). An exploration
ratio α ∈ [0, 1] enables to balance between exploration and
exploitation, by scaling up or down the amount of sampled
programs relative to the amount of enumerated programs at
each iteration. While the enumeration procedure aims to
find the most efficient programs (via posterior-optimisation),
the sampling procedure enables wider exploration, by relaxing this 
optimisation into a minimal criteria on the posterior: P(ρ | L, z) ≥ δ
ef f . Adjusting the exploration ratio
α, and the sampling temperature parameter τ (Boltzman Exploration) 
enables to balance the efficiency pressure and the
novelty pressure, as illustrated in our experiments
  </p>


  <p>
    <strong>Novelty Selection</strong> At each iteration, the most
    novel programs are selected from previously generated programs (top down and bottom up frontiers), 
    to form the elites
    E, under the condition that their novelty score is above a
novelty threshold δ nov. The local novelty score of a program ρ in a niche z 
is measured as the k-mean of the phenotypic
distance to the archive elements of the same niche.
Top-N programs are selected one-by-one for the sampled
niche, and at each step, the novelty score of each candidate
is re-evaluated to adapt to the growing niche.
To capture interesting variations between structures, instead of a naive pixel-based
distance, we rely on a pre-trained visual network6
, illustrated Figure 3. The phenotypic features fρ ∈ R d of a program ρ
are defined as the visual CNN-features of the block structure
it corresponds to (obtained after executing the program and
rendering it as an image), using a pretrained CNN (VGG 16
Simonyan and Zisserman (2014)). The phenotypic distance
is then defined as the euclidean distance between the CNN
features:
  </p>


  <p>
    <strong>Abstraction</strong> In essence, the abstraction phase aims to
    grow the library from useful and novel building blocks found
    in elites programs across niches. The abstraction phase,
    first refactors the elites to find common program fragments,
    before abstracting these into new programs
    <d-footnote>This sequestration of information into reusable units –
      abstractions– is a form of <i>knowledge distillation.</i></d-footnote>
    . These abstracted programs –referred to as primitives (to distinguish
    them from generated programs)– are functions with maximum 3 arguments in our experiments. It relies on the selfsupervised compression algorithm from DreamCoder (Ellis
    et al., 2021) (cf. Appendix S4.5) which aims to maximize
    a Bayesian criterion, through a generalised Expectation Maximisation algorithm.
    To encourage a more evolving library, soften the language-bias, and incorporate additional stochasticity
    <d-footnote>Stochasticity, instabilities, and the possibility of historical contingencies seems to be tied to the emergence of innovation, as argued in Taylor et al. (2016).</d-footnote>
    , OED includes a stochastic pruning mechanism, weighted by innovation persistence:
    every iteration, a weighted sampling of ≤ β primitives to
    prune is performed (β = 2, 3 in our experiments). The sampling weights corresponds to the activity score of each primitive at the current iteration, as defined in Equation 6. We
    also incorporate a novelty-guided library minimal criteria,
    enforcing the new primitives to be distant enough from the
    existing primitives, relying on a point-based approximation
    defined in Equation 5 and illustrated Figure 3.
    <d-footnote>Distance between primitives (function): first,
      we sample for each primitive a set Cˆ
      p of mp ≤ 20 programs  obtained by instantiating p on arguments ∈ {0, 9}. 
      The approximate distance between primitives is then defined as the
      mean over programs in p-cloud of the distance to the p
      ′−cloud.</d-footnote>
    
  </p>


  <p>
    <strong>Abstraction</strong> The neural model, illustrated in Figure 2, aims to learn how to compose the programmatic abstraction to create novel programs. It encodes a distribution
    over the elements in the library conditioned on the local context: taking as input a latent niche vector z, and the current
    state s passed as an image (e.g. image of the current tower
being constructed), it outputs a probability tensor encoding
the probability of a next token being j given the previous token in the growing language. Querying the neural model Q
auto-regressively produces a program. It is trained both on
replays –i.e. elites from previous iterations– and dreams –i.e
programs sampled from the generative model, with a default
dream ratio of 0.5. It plays the crucial role of biasing the
search space, as it drastically reduces the breadth of search
in the combinatorially exploding space of programs.

  </p>

  <h2 id="experiment-0">Experiments: Tower Domain</h2>
  <p>
    We demonstrate our algorithm on the
Tower Domain introduced in DreamCoder (Ellis et al.
(2021), Appendix S2.1.4). We rely on the same monadic,
continuation-passing encoding of imperative programs. In
this domain, the programs produce a sequence of actions,
which encode the instruction of a hand moving over a 2D
discretized canvas, which can drop horizontal or vertical
blocks. The state of the hand includes both a 1D position and a boolean orientation. By default, the initial Library is seeded with two basic control flow primitives: for
(for loop), and get/set which gets and saves the current
state of the agent’s hand. We also provide movement-related
and construction-related primitives: moveHand (advancing the hand), reverseHand (flipping the orientation),
dropVerticalBlock and dropHorizontalBlock.
To define the niches, we adopt a 3D behavioral characteristic, consisting of (1) center of gravity (y-coordinate) (2)
width, and (3) number of blocks of the tower structure generated from a program. The ecosystem minimal criteria consists of lower and upper bounds to these values (e.g. 0−120,
0 − 240 and 10 − 400).
  </p>


  <h2 id="experiment-1">Experiment 1: Bias Experiment</h2>


  <p>
    The emphasis on implementation nested in the notion of innovation implies the candidate
novel artifacts-ideas will likely undergo strong environmental pressures (e.g. market pressure, functional pressures, social pressures etc.) when being put at test in the real world,
heavily biasing the search space. It seems therefore legitimate to wonder if strong inductive biases and environmental
coupling are key to guide the emergence of abstract structured knowledge and innovation. Generating and retaining
’stepping stones to everywhere’ seems indeed too ambitious
in such a combinatorially exploding space of programs in
the absence of any environmental pressure (what survive,
what functions, etc.). Combining such quest with some inductive bias appears to be a more realistic reflection of how
symbolic knowledge formation occurs in social and natural
systems. Moreover, it holds the potential for yielding more
reliable and consistent innovations. We investigate different
ways to bias evolution and guide the formation of symbolic
knowledge, through either: . (INNATE)-bootstrapped Library, initially seeded with 5 primitives, 
(PHYSICS) a gravity-aware enviroment, enacting strong environmental pressure,
(CNN)-pretrained distance metric for novelty evaluation.

  </p>

  <h2 id="experiment-2">Experiment 2: Novelty-Diversity Trade Off</h2>

  <p>
  </p><figure style="
      margin-left: auto;
      margin-right: auto;
      grid-column: page;
      width: 100%;
      max-width: 2000px;
    ">
    <img src="figures/Pressures.png" style="width: 600px">
    <figcaption> The environmental, language and novelty pres-
      sures at play in the selection of generated programs at each
      iteration. (1) Legit programs (grammatically correct) un-
      dergo environmental pressures (e.g. niches and minimal cri-
      teria on the number of blocks, width, etc.), enclosed in the
      likelihood P(z | ρ); (2) Candidates programs undergo a
      language-pressure, deriving from the learned library L, en-
      acted through the prior P(ρ | L); (3) finally, generated pro-
      grams are subjected to a novelty selection.</figcaption>
  </figure>
<p></p>
  <p>
    As illustrated Figure 8, at the core of OED search for
diversity-enabling programs –and at the core of open-
endedness– lies a delicate balance between environmental,
efficiency and novelty pressures. These different pressures
at play to select elites at each iteration may be conflictive:
e.g. novel solutions compared to the archive commonly
stands out as less likely given the library (therefore, less ef-
ficient) and conversely, most efficient programs (minimising
the prior P(ρ | L)), often are less novel
  </p>
  <p>
    <strong>Convergent Evolution</strong> Strong efficiency pressures, either
    from environmental constraints (cf. (PHYSICS)), or via
    stronger language-pressure (cf. (BASE)), or conversely
    lower novelty pressure (as in the (A-N) run), seem to favor
    phenomenon of convergent evolution, i.e. the repeatability
    of certain evolutionary outcomes. Indeed, certain programs
    and primitives were (approximately) reinvented across dif-
    ferent evolutionary runs, as they appeared to be some of the
    most efficient way to inhabit certain ecological niches and
    fit the minimal criteria (notably when this one is more de-
    manding as in the gravity-aware environment): e.g. slanted
    lines, cranes-type structure, parallelograms, comb-like arti-
    fact, wall-type of structures, and combinations of them (as
    displayed in Figure 5) were invented across multiple runs

  </p>

  <p>
    <strong>Enhanced Exploration</strong> 
    As qualitatively reflected in the
programs invented or abstracted (Figures 4, Figure 5), learn-
ing a language also implies learning a strong inductive bias
towards the search space. This language bias is notably en-
acted through the prior P(ρ | L) pressure in the generation
step. Under a strong language bias, early primitives incor-
porated in the library (e.g. the noticeable ”zigzag” artifact in
the (BASE) run added to the library at the second iteration,
cf. Figure 5) have a strong effect on the generation of new
programs in later runs.
To mitigate such effect, OED provide ways to loosen the
efficiency pressure in favor of the novelty pressure, by pro-
moting exploration in the generation step, notably through
increasing the exploration ratio α weighting the sampling
or the sampling temperature τ 9. As qualitatively reflected
Figure 9, early experiments on this enhanced exploration
confirm that more diverse and novel programs are gener-
ated. However, by reducing the presence of regularities
among elites, excessive exploration can impede the abstrac-
tion phase and therefore the library growing

  </p>
  
  <h2 id="experiment-3">Experiment 3: Ablation Experiment</h2>
  <p>
    To assess the benefit of the different components of our
    model in the open-ended creation of diverse programs,
    we run several ablation studies both (A-L) without the li-
    brary, (A-R) without the neural guidance, (A-N) without the
    novelty-Selection phase. Comparative results are presented
    in Figure 10. Qualitative results in Figure 4, 5 confirms
    quantitative results in terms of the overall impoverishment
    of found solutions in these ablation experiments.
  </p>

  
  <h2 id="discussion">Conclusion</h2>

  <p>
    While preliminary, our results suggest that neurally-guided
library learning, enabling both more efficient program dis-
covery and more sustained diversity is a promising framing
towards open-endedness. By building a library of program-
matic abstractions —instead of solutions—, our method
preserves regularities in a more potent and versatile way than
other diversity-oriented approaches. As we have observed,
OED has the potential to guide the development of sym-
bolic knowledge using innate priors, biased metrics, and en-
vironmental constraints. Accentuating efficiency pressures,
such as language-bias and environmental factors, led to phe-
nomenon of convergent evolution. Library learning entails
a growing amount of bias, which may have a negative im-
pact on diversity. To mitigate this effect, and foster cre-
ative divergence, it seems crucial to promote exploration
and stochasticity, both during the generation and abstraction
phases. However, by reducing the presence of regularities
among elites, excessive exploration may impede the abstrac-
tion mechanism. Further investigation is necessary to deter-
mine the optimal balance between exploration and efficiency
in library learning, which could be automatically adjusted
depending on the task. The simplicity of the Tower Domain
task do not allow OED to reach its full potential for open-
ended creation. To unleash its full capabilities, and the bene-
fit of novelty-oriented library learning model in downstream
tasks, OED should be deployed in richer environments with
stronger environmental coupling and functional objectives.
Another compelling development would be to use OED for
the production of increasingly sophisticated behaviors, in-
stead of simply artifacts.
  </p>

</d-article>

<d-appendix>

  <h3>Acknowledgments</h3>
  <p>
    This project was funded by a DFF-Research Project grant
(9131 − 00042B)
  </p>



   <!--AUTHOR CONTRIB
  <h3>Author Contributions</h3>
  <p>
    <strong>Research:</strong> Alexander came up with the Self-Organising
    Asynchronous Neural Cellular Automata model and Ettore contributed to its
    design. Ettore designed and performed most of the experiments for this work.
    Alexander supervised the entire process and contributed extensively to the
    later stages of development by performing experiments and refining the
    model.
  </p>

  <p>
    The idea of applying neural networks to understanding regeneration, and to
    designing self-organising systems, was proposed by Michael Levin in his
    email to Alexander, that was sent following the DeepDream
    <d-cite key="mordvintsev2015inceptionism"></d-cite> publication by Alexander
    in 2015. Alexander’s proposal of this model and this work were inspired by
    the talk <d-cite key="WhatBodiesThink"></d-cite> given by Michael at NeurIPS
    2018 as well as the subsequent email exchange between Alexander and Michael.
  </p>

  <p>
    <strong>Writing &amp; Diagrams:</strong> Alexander outlined the structure of the
    article, and contributed to the content throughout. Ettore contributed to
    the content throughout. Eyvind drew all the diagrams, contributed to the
    content throughout, and wrote all of the pseudocode. Michael made extensive
    contributions to the article text, providing the biological context and
    motivation for this work.
  </p>
   -->
<!-- This is a comment in HTML -->

  <!--FOOTNOTES -->
  <d-footnote-list></d-footnote-list>

  <!--REFERENCES-->
  <d-citation-list distill-prerendered="true">
  <h3 id="references">References</h3><ol id="references-list" class="references"><li id="ellis2021dreamcoder"><span class="title">Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning.</span> <br>Ellis, K., Wong, C., Nye, M., Sable-Meyer, M., Morales, L., Hewitt, L., Cary, L., Solar-Lezama, A., and Tenenbaum, J. B, 2021. . In Proceedings of the 42nd acm sigplan international conference on programming language design and implementation, pages 835–850.</li>
   </ol>
  </d-citation-list>

</d-appendix>
</body>

<!--FOOTER
<distill-footer>
<div class="footer-container">
  <div class="nav">
    <a href="https://github.com/distillpub">GitHub</a>
  </div>
</div>

</distill-footer>
->